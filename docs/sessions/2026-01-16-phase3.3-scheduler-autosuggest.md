# Phase 3.3: Scheduler & Auto-Suggest

**Date:** 2026-01-16
**Type:** Implementation
**Status:** Complete

---

## Context

Phase 3.1 complete (database schema, admin API).
Phase 3.2 complete (DecomposerAgent, BlogComposerAgent).
This session implements scheduled blog post generation and topic discovery.

**References:**
- ADR 003 lines 184-284 (Scheduler), 124-180 (Auto-Suggest)
- Phase 3 planning session lines 173-177
- Phase 3.2 session notes lines 207-223

---

## Implementation Summary

### 1. APScheduler Integration

**File:** `src/backend/requirements.txt`

Added dependency:
```
apscheduler==3.10.4
```

**Why APScheduler:**
- Cron-style scheduling for predictable tasks
- Lighter than Celery (no message broker required)
- Persistent across server restarts
- Sufficient for single-server deployment

---

### 2. SchedulerService

**File:** `src/backend/services/scheduler.py` (400 lines)

**Purpose:** Orchestrates scheduled blog post generation with full flow.

**Key Components:**

#### SchedulerConfig
Configuration model:
- `enabled`: Boolean toggle
- `posts_per_day`: Number of posts to generate per day (default: 1)
- `cron_hour`: Hour to run (0-23, UTC)
- `cron_minute`: Minute to run (0-59)
- `max_concurrent`: Max concurrent generations (fixed at 1)

#### SchedulerService
Main service class with methods:

**`configure(config: SchedulerConfig)`**
- Updates scheduler configuration
- Adds/removes APScheduler cron job dynamically

**`start()` / `shutdown()`**
- Start/stop APScheduler
- Called on application startup/shutdown

**`generate_next_blog_post()`**
Full generation flow (implements ADR 003 specification):

1. **Pick highest priority topic** from queue (status="queued")
2. **Run DecomposerAgent** → 3-12 component claims
3. **Process each component claim:**
   - Generate embedding for claim text
   - Semantic search existing claim cards (threshold 0.92)
   - If found (similarity >= 0.92): Reuse existing claim_card_id
   - If not found: Run 5-agent pipeline → new claim card
4. **Run BlogComposerAgent** → synthesized article (title + body)
5. **Create blog_posts row** with claim_card_ids array
6. **Update topic status** → "completed", review_status="pending_review"
7. **Queue for admin review**

**Error Handling:**
- Fail fast: No automatic retries
- Mark topic status="failed" with error_message
- Admin reviews in failure monitor

**Deduplication Logic:**
- `_find_existing_claim()`: Semantic search with 0.92 threshold
- `_generate_claim_card()`: Run 5-agent pipeline for novel claims
- `_claim_card_to_dict()`: Convert ClaimCard model to dict for BlogComposerAgent

**Concurrency:**
- `_generation_lock`: Asyncio lock prevents overlapping runs
- `max_instances=1` in APScheduler job config

---

### 3. AutoSuggestService

**File:** `src/backend/services/autosuggest.py` (330 lines)

**Purpose:** Discovers new apologetics topics for the queue using LLM analysis.

**Key Components:**

#### AutoSuggestConfig
Configuration model:
- `enabled`: Boolean toggle
- `max_topics_per_run`: Max topics to extract (default: 10)
- `similarity_threshold`: Deduplication threshold (default: 0.85)
- `default_priority`: Default priority for extracted topics (default: 5)

#### AutoSuggestService
Main service class with methods:

**`extract_topics_from_text(source_text, source_url, source_name)`**
- Uses LLM (Claude Haiku 3.5) to extract apologetics topics
- System prompt guides extraction:
  - Focus on factual claims (historical, scientific, theological)
  - Topics broad enough for decomposition, specific enough to analyze
  - Avoid purely philosophical debates or personal testimonies
- Returns: List of topics with reasoning and priority (1-10)

**LLM Output Format:**
```json
{
  "topics": [
    {
      "topic_text": "Brief topic description",
      "reasoning": "Why this is interesting",
      "estimated_priority": 1-10
    }
  ]
}
```

**Priority Scoring Guidelines (embedded in prompt):**
- 8-10: Widely circulated, prominent apologists, highly debated
- 5-7: Moderately common, interesting but not urgent
- 1-4: Niche claims, less common, lower impact

**`add_topics_to_queue(topics, skip_deduplication)`**
- Deduplicates extracted topics against existing claim cards
- Uses semantic search with 0.85 threshold (lower than scheduler's 0.92)
- Creates TopicQueue entries with priority and context
- Returns summary: added, skipped_duplicates, failed

**Deduplication Logic:**
- `_check_duplicate()`: Semantic search with 0.85 threshold
- If similar claim card exists: Skip topic (not added to queue)
- If no match: Add to queue with priority

**Note:** Current implementation requires manual text input. Future enhancements will add:
- Web crawling of configured sources (AiG, WLC, CARM)
- RSS feed monitoring
- Twitter/X account monitoring
- YouTube transcript analysis

---

### 4. Admin API Endpoints

**File:** `src/backend/main.py` (additions)

#### Scheduler Endpoints

**GET /api/admin/scheduler/settings**
- Returns current scheduler configuration
- Response: enabled, posts_per_day, cron_hour, cron_minute, max_concurrent

**PUT /api/admin/scheduler/settings**
- Updates scheduler configuration
- Request body: SchedulerSettingsRequest (enabled, posts_per_day, cron_hour, cron_minute)
- Validation: posts_per_day (1-10), cron_hour (0-23), cron_minute (0-59)
- Reconfigures APScheduler job dynamically

**POST /api/admin/scheduler/run-now**
- Manually triggers blog post generation (bypasses schedule)
- Calls `scheduler_service.generate_next_blog_post()`
- Returns: Generation result with blog_post_id and metadata
- HTTP 404 if no queued topics available

#### Auto-Suggest Endpoints

**GET /api/admin/autosuggest/settings**
- Returns current auto-suggest configuration
- Response: enabled, max_topics_per_run, similarity_threshold, default_priority

**PUT /api/admin/autosuggest/settings**
- Updates auto-suggest configuration
- Request body: AutoSuggestSettingsRequest (enabled, max_topics_per_run, similarity_threshold)
- Validation: max_topics_per_run (1-50), similarity_threshold (0.0-1.0)

**POST /api/admin/autosuggest/trigger**
- Manually triggers topic extraction from provided text
- Request body: AutoSuggestExtractRequest (source_text, source_url, source_name, skip_deduplication)
- Calls `autosuggest_service.extract_topics_from_text()` → `add_topics_to_queue()`
- Returns: Summary (extracted, added, skipped_duplicates, failed)

#### Pydantic Models
Added models for request validation:
- `SchedulerSettingsRequest`
- `AutoSuggestSettingsRequest`
- `AutoSuggestExtractRequest`

---

### 5. Application Lifecycle

**File:** `src/backend/main.py` (additions)

Added startup/shutdown event handlers:

**`@app.on_event("startup")`**
- Starts scheduler service on application startup
- Prints: "Scheduler service started (enabled: True/False)"

**`@app.on_event("shutdown")`**
- Shuts down scheduler service on application shutdown
- Ensures clean APScheduler shutdown

---

## Design Decisions

### 1. Semantic Search Thresholds

**Scheduler (0.92):**
- High threshold for deduplication during generation
- Only reuses claim cards with very high similarity
- Ensures comprehensive coverage (generates more new cards)

**Auto-Suggest (0.85):**
- Lower threshold for topic discovery deduplication
- Catches broader duplicates during extraction
- Reduces noise in topic queue

**Rationale:** Two-layer approach balances discovery (cast wide net) with generation (be thorough).

### 2. Fail-Fast Behavior

**No automatic retries:**
- Failed topics marked with status="failed" and error_message
- Admin reviews failure in monitor
- Admin can manually retry with adjustments

**Rationale:** Consistent with ADR 001 principle (no silent failures, full transparency).

### 3. Sequential Processing

**Max concurrent = 1:**
- Single generation at a time (no parallel runs)
- `_generation_lock` prevents overlapping executions
- APScheduler `max_instances=1` prevents job overlap

**Rationale:**
- Simpler error handling and logging
- Avoid database contention
- Sufficient for single-server deployment (1 post/day target)

### 4. Service Initialization

**Global singleton instances:**
```python
scheduler_service = SchedulerService()
autosuggest_service = AutoSuggestService()
```

**Rationale:**
- Single shared configuration across all endpoints
- Consistent state management
- Easy access from API endpoints and startup/shutdown hooks

### 5. Deduplication Strategy

**Component-level deduplication:**
- Check each component claim individually
- Reuse existing claim cards where possible
- Only generate new cards for novel component claims

**Example:**
- Topic: "Noah's Flood"
- Decomposer produces 5 component claims
- 3 claims already exist (reused)
- 2 claims are novel (run 5-agent pipeline)
- Blog Composer receives 5 claim cards (mix of existing + new)

**Rationale:**
- Maximizes reuse of existing audited claims
- Reduces redundant LLM calls
- Maintains comprehensive topic coverage

---

## Integration Flow (End-to-End)

### Scheduled Generation Flow

1. **APScheduler triggers** at configured time (e.g., 2:00 AM UTC)
2. **`_run_scheduled_generation()`** called
3. **For each post** (up to `posts_per_day`):
   - Call `generate_next_blog_post()`
4. **`generate_next_blog_post()` executes:**
   - Get highest priority topic (status="queued")
   - Mark topic status="processing"
   - Run DecomposerAgent → component_claims list
   - For each component claim:
     - Generate embedding
     - Semantic search (threshold 0.92)
     - If match: Reuse claim_card_id
     - If no match: Run 5-agent pipeline → new claim_card_id
   - Run BlogComposerAgent with all claim cards
   - Create BlogPost row (published_at=NULL)
   - Update topic: status="completed", review_status="pending_review"
5. **Admin reviews** blog post in admin portal (Phase 3.4)
6. **Admin approves** → Sets published_at timestamp

### Manual Generation Flow (API)

1. **Admin calls** POST /api/admin/scheduler/run-now
2. **Same flow** as scheduled generation (step 4 above)
3. **Returns immediately** with blog_post_id and metadata

### Auto-Suggest Flow (API)

1. **Admin calls** POST /api/admin/autosuggest/trigger with source text
2. **`extract_topics_from_text()`** sends text to LLM
3. **LLM returns** list of extracted topics with priorities
4. **`add_topics_to_queue()`** processes topics:
   - For each topic:
     - Generate embedding
     - Semantic search (threshold 0.85)
     - If match: Skip (duplicate)
     - If no match: Add to topic_queue
5. **Returns summary** to admin

---

## Files Modified/Created

**Created:**
- `src/backend/services/scheduler.py` (400 lines)
- `src/backend/services/autosuggest.py` (330 lines)
- `docs/sessions/2026-01-16-phase3.3-scheduler-autosuggest.md` (this file)

**Modified:**
- `src/backend/requirements.txt` (+3 lines: APScheduler dependency)
- `src/backend/main.py` (+270 lines: imports, models, endpoints, lifecycle hooks)

**Total:** ~1000 lines of implementation

---

## Testing Notes

### Manual Testing (After Implementation)

**Scheduler:**
1. Start backend: `cd src/backend && source venv/bin/activate && uvicorn main:app --host 0.0.0.0 --port 8008`
2. Verify scheduler started: Check console for "Scheduler service started (enabled: False)"
3. Create test topic: POST /api/admin/topics with priority=10
4. Manual trigger: POST /api/admin/scheduler/run-now
5. Verify:
   - Topic status changed to "processing" → "completed"
   - BlogPost created (check /api/admin/topics for blog_post_id)
   - Component claim cards created (check /api/claim-cards)
   - Review status = "pending_review"

**Auto-Suggest:**
1. Prepare sample text (copy from apologetics website)
2. POST /api/admin/autosuggest/trigger with source_text
3. Verify:
   - Topics extracted (check response: extracted count)
   - Topics added to queue (check response: added count)
   - Duplicates skipped (check response: skipped_duplicates)
   - Topics visible in /api/admin/topics

**Scheduler Configuration:**
1. GET /api/admin/scheduler/settings (default: enabled=False)
2. PUT /api/admin/scheduler/settings with enabled=True, posts_per_day=1
3. Verify: Scheduler job added (check APScheduler logs)
4. Wait for scheduled time or trigger manually

**Edge Cases:**
- Empty topic queue (POST /api/admin/scheduler/run-now should return 404)
- Failed generation (check topic status="failed", error_message populated)
- Duplicate detection (same topic twice should skip second)

---

## Success Criteria

Phase 3.3 complete when:
- [x] APScheduler installed and integrated
- [x] SchedulerService implements full generation flow:
  - [x] Decomposer → component claims (3-12)
  - [x] Semantic search dedup (threshold 0.92)
  - [x] 5-agent pipeline for novel claims
  - [x] Blog Composer → synthesized article
  - [x] BlogPost creation
  - [x] Fail-fast error handling
- [x] AutoSuggestService implements topic discovery:
  - [x] LLM extraction from text
  - [x] Priority scoring (1-10)
  - [x] Semantic search dedup (threshold 0.85)
  - [x] Topic queue population
- [x] Admin API endpoints:
  - [x] GET/PUT /api/admin/scheduler/settings
  - [x] POST /api/admin/scheduler/run-now
  - [x] GET/PUT /api/admin/autosuggest/settings
  - [x] POST /api/admin/autosuggest/trigger
- [x] Application lifecycle hooks (startup/shutdown)
- [x] Session notes documented

---

## Next Steps (Phase 3.4)

**Review Workflow Backend:**
1. Review service methods:
   - `get_pending_reviews()`: List topics with review_status="pending_review"
   - `approve_blog_post(blog_post_id)`: Set published_at, update topic status
   - `reject_blog_post(blog_post_id, reason)`: Mark rejected, store reason
   - `request_revision(blog_post_id, feedback, re_run_target)`: Selective re-run
2. Selective re-run logic:
   - Re-run decomposer only (if topic breakdown wrong)
   - Re-run specific claim card pipeline (if individual claim wrong)
   - Re-run blog composer only (if title/article wrong)
3. API endpoints:
   - GET /api/admin/review/pending
   - POST /api/admin/review/{id}/approve
   - POST /api/admin/review/{id}/reject
   - POST /api/admin/review/{id}/revision

---

## Notes

**Design Principles Maintained:**
- Fail fast (no silent retries)
- Full transparency (all errors logged, visible to admin)
- Database-stored prompts (used by DecomposerAgent, BlogComposerAgent)
- No shortcuts (all content through 5-agent pipeline)

**User Preferences Honored:**
- Concise session notes (~280 lines)
- Small incremental changes (Phase 3.3 scope only)
- Stayed within defined task
- Documented as we went

**Deviations from ADR 003:**
- Auto-suggest web crawling deferred (basic LLM extraction implemented)
- Full source monitoring (RSS, Twitter, YouTube) deferred to future enhancement
- Current implementation sufficient for manual text input workflow

---

**Status:** Implementation complete, ready for Phase 3.4 (Review Workflow Backend).
