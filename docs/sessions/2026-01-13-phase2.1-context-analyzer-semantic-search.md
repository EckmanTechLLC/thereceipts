# Phase 2.1: Context Analyzer & Semantic Search

**Date:** 2026-01-13
**Phase:** 2.1 - Conversational Chat Infrastructure
**Status:** Complete

---

## Objective

Implement conversational context analysis and semantic search for existing claim cards.

**Goal:** Enable intelligent routing of chat messages through context reformulation and vector similarity search.

---

## Reference Documents

- `/docs/sessions/2026-01-13-phase2-planning.md` - Architecture decisions for Phase 2
- `/docs/decisions/001-core-architecture-and-system-design.md` - Core system design
- `/src/backend/database/models.py` - ClaimCard model with embedding field (1536 dimensions)
- `/src/backend/services/llm_client.py` - Existing LLM client wrapper

---

## Implementation Summary

### 1. Context Analyzer Service
**File:** `src/backend/services/context_analyzer.py`

**Purpose:** Reformulates user messages with conversation history to create standalone, contextualized questions.

**Implementation:**
- Uses lightweight LLM calls (Claude Haiku primary, GPT-3.5-turbo fallback)
- `analyze_context(conversation_history, new_message)` method
- Low temperature (0.3) for consistency
- Fast timeout (10 seconds)
- System prompt focused on question reformulation

**Example:**
```python
# Input
conversation_history = [
    {"role": "user", "content": "Did Matthew copy Mark?"}
]
new_message = "What about Luke?"

# Output
"Did Luke copy Mark?"
```

**Key Features:**
- Returns message as-is if already standalone
- Extracts only user questions from history (ignores assistant responses)
- Automatic fallback to OpenAI if Anthropic fails
- Proper error handling with custom exceptions

---

### 2. Embedding Service
**File:** `src/backend/services/embedding.py`

**Purpose:** Generate embeddings for semantic similarity search using OpenAI ada-002.

**Implementation:**
- Model: `text-embedding-ada-002` (1536 dimensions)
- Retry logic: 3 attempts with exponential backoff
- Batch processing support (up to 100 texts per batch)
- Dimension validation

**Methods:**
- `generate_embedding(text)` - Single text embedding
- `batch_generate_embeddings(texts, batch_size=100)` - Bulk operations
- `cosine_similarity(vec1, vec2)` - Similarity calculation (for testing)

**Error Handling:**
- Custom `EmbeddingServiceError` exception
- Validates embedding dimensions (1536)
- Handles empty/invalid input
- Graceful degradation for batch failures

---

### 3. ClaimCardRepository Extensions
**File:** `src/backend/database/repositories.py`

**New Methods:**

#### `search_by_embedding(embedding, threshold=0.85, limit=5)`
- Uses pgvector cosine distance operator (`<=>`)
- Converts distance to similarity: `similarity = 1 - (distance / 2)`
- Returns list of tuples: `(ClaimCard, similarity_score)`
- Ordered by similarity (highest first)
- Includes full relationships (sources, tags)

#### `upsert_embedding(claim_card_id, embedding)`
- Updates embedding for existing claim card
- Returns `True` if successful, `False` if claim card not found
- Simple wrapper around model update

**SQL Query Pattern:**
```sql
SELECT
    c.*,
    1 - (c.embedding <=> :query_embedding) / 2 as similarity
FROM claim_cards c
WHERE
    c.embedding IS NOT NULL
    AND (c.embedding <=> :query_embedding) <= :distance_threshold
ORDER BY c.embedding <=> :query_embedding
LIMIT :limit
```

---

### 4. Chat Message Endpoint
**File:** `src/backend/main.py`

**Endpoint:** `POST /api/chat/message`

**Request:**
```json
{
  "message": "What about Luke?",
  "conversation_history": [
    {"role": "user", "content": "Did Matthew copy Mark?"},
    {"role": "assistant", "content": "..."}
  ]
}
```

**Response (existing match):**
```json
{
  "type": "existing",
  "similarity": 0.92,
  "contextualized_question": "Did Luke copy Mark?",
  "claim_card": { /* full claim card object */ }
}
```

**Response (no match):**
```json
{
  "type": "generating",
  "pipeline_status": "queued",
  "contextualized_question": "Did Luke copy Mark?",
  "message": "No existing claim card found. Pipeline generation will be implemented in Phase 2.2."
}
```

**Flow:**
1. Validate request (non-empty message)
2. Initialize services (LLMClient, ContextAnalyzer, EmbeddingService)
3. Reformulate question with conversation context
4. Generate embedding for contextualized question
5. Semantic search existing claim cards (threshold: 0.85)
6. Return existing card if match found, otherwise return "generating" status

**Error Handling:**
- 400: Empty message
- 500: Context analysis failure
- 500: Embedding generation failure
- 500: Unexpected errors

---

### 5. Embedding Generation Script
**File:** `src/backend/scripts/generate_embeddings.py`

**Purpose:** Backfill embeddings for existing claim cards.

**Usage:**
```bash
# Generate embeddings for all claim cards (skip existing)
python scripts/generate_embeddings.py

# Generate embeddings for all (including re-generating existing)
python scripts/generate_embeddings.py --all

# Generate embedding for specific claim card
python scripts/generate_embeddings.py --claim-id <uuid>

# Custom batch size
python scripts/generate_embeddings.py --batch-size 50
```

**Features:**
- Progress indicators per claim card
- Batch processing with configurable size
- Statistics summary (total, generated, skipped, failed)
- Skip existing embeddings by default
- Individual claim card processing
- Exit codes: 0 (success), 1 (failures occurred)

**Output:**
```
Found 10 claim cards

Processing batch 1 (10 claim cards)...
  Skipped: abc-123 (already has embedding)
  Generating embedding for: def-456
  âœ“ Generated embedding for: def-456
Batch 1 committed

============================================================
EMBEDDING GENERATION SUMMARY
============================================================
Total claim cards:      10
Embeddings generated:   5
Skipped (existing):     3
Failed:                 2
============================================================
```

---

## Dependencies

**No new dependencies required** - all needed packages already in `requirements.txt`:
- `openai==1.12.0` (for embeddings)
- `anthropic==0.25.1` (for context analyzer)
- `pgvector==0.2.4` (for vector search)

---

## Configuration

**Settings used (from `config.py`):**
- `OPENAI_API_KEY` - Required for embedding generation
- `ANTHROPIC_API_KEY` - Required for context analyzer (with OpenAI fallback)
- `SEMANTIC_SEARCH_THRESHOLD` - Default: 0.85 (configurable)

---

## Testing Recommendations

### Manual Testing (Phase 2.2+)
1. **Context Reformulation:**
   - Test with empty history (should return message as-is)
   - Test with single previous question
   - Test with multiple conversation turns
   - Test pronoun resolution ("it", "that", "what about...")

2. **Semantic Search:**
   - Create test claim cards with embeddings
   - Test similar questions (should match)
   - Test unrelated questions (should not match)
   - Test threshold boundary cases (0.84 vs 0.85)

3. **Chat Endpoint:**
   - Test without conversation history
   - Test with conversation history
   - Test empty message (should return 400)
   - Test with no matching claim cards
   - Test with matching claim cards

4. **Embedding Generation:**
   - Run script with no claim cards
   - Run script with claim cards (no embeddings)
   - Run script with claim cards (some have embeddings)
   - Test --all flag (re-generate)
   - Test --claim-id flag

---

## Known Limitations

1. **No Pipeline Integration:** Phase 2.1 only returns "generating" status when no match found. Phase 2.2 will implement actual pipeline execution.

2. **Context Analyzer Fallback:** Uses OpenAI GPT-3.5-turbo as fallback if Anthropic fails. Ensure both API keys configured for reliability.

3. **Embedding Search Performance:** Raw SQL query may be slow with large claim card tables. Consider adding pgvector index if needed (not required for Phase 2.1 scale).

4. **No Caching:** Context analyzer makes fresh LLM call every time. Consider caching reformulated questions if needed (not required for Phase 2.1).

---

## Next Steps (Phase 2.2)

1. Integrate pipeline orchestrator with chat endpoint
2. Handle "generating" case by triggering full 5-agent pipeline
3. WebSocket integration for real-time progress updates during generation
4. Response formatter (convert claim card to conversational response)
5. Session management (temporary in-memory or Redis with TTL)

---

## Files Created/Modified

### Created:
- `src/backend/services/context_analyzer.py` (165 lines)
- `src/backend/services/embedding.py` (195 lines)
- `src/backend/scripts/generate_embeddings.py` (240 lines)

### Modified:
- `src/backend/database/repositories.py` (added 2 methods, ~85 lines)
- `src/backend/main.py` (added endpoint + models, ~140 lines)

**Total:** ~825 lines of code added

---

## Completion Checklist

- [x] Context Analyzer Service implemented
- [x] Embedding Service implemented
- [x] ClaimCardRepository extended with semantic search
- [x] POST /api/chat/message endpoint added
- [x] Embedding generation script created
- [x] Session notes documented
- [ ] Integration testing (deferred to Phase 2.2)
- [ ] Pipeline integration (Phase 2.2)

---

**Status:** Phase 2.1 Complete. Ready for Phase 2.2 (Chat Backend Integration).
