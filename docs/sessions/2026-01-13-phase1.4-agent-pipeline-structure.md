# Session: Phase 1.4 - Agent Pipeline Structure

**Date:** 2026-01-13
**Phase:** 1.4 Foundation
**Status:** Complete

---

## Objective

Implement 5-agent pipeline infrastructure with LLM integration. Focus on structure and orchestration, not full claim card generation yet (that's Phase 2).

---

## What Was Built

### 1. LLM Client Wrapper (`src/backend/services/llm_client.py`)
- Unified interface for Anthropic and OpenAI APIs
- `call_anthropic()` - Claude models
- `call_openai()` - GPT models
- `call()` - Unified interface with provider selection
- Proper timeout handling (defaults to `PIPELINE_TIMEOUT` from settings)
- Custom exceptions: `LLMClientError`, `LLMTimeoutError`, `LLMProviderError`
- Async/await throughout

### 2. Base Agent Class (`src/backend/agents/base.py`)
- Abstract base class for all agents
- `load_config()` - Loads agent configuration from `agent_prompts` table
- `call_llm()` - Calls LLM with configured provider/model/params
- `execute()` - Abstract method for agent-specific logic
- `run()` - Main entry point (loads config → executes → returns structured result)
- Fail-fast behavior: No retries, immediate error propagation
- Custom exceptions: `AgentError`, `AgentConfigurationError`, `AgentExecutionError`

### 3. Five Agent Implementations

#### TopicFinderAgent (`agents/topic_finder.py`)
- Identifies claim, claimant, claim_type, context from question
- Returns structured JSON with required fields
- Validates output completeness

#### SourceCheckerAgent (`agents/source_checker.py`)
- Collects primary historical sources
- Collects scholarly peer-reviewed sources
- Returns evidence summary
- Clear distinction between source types

#### AdversarialCheckerAgent (`agents/adversarial_checker.py`)
- Attempts to falsify analysis
- Determines verdict (True/Misleading/False/Unfalsifiable/Depends)
- Assesses confidence level (High/Medium/Low) with explanation
- Identifies apologetics techniques used
- Verifies quotes and sources

#### WritingAgent (`agents/writing_agent.py`)
- Produces short_answer (≤150 words, plain language)
- Produces deep_answer (3-5 paragraphs, forensic tone)
- Identifies why_persists (psychological/social/institutional reasons)
- Validates word count constraints

#### PublisherAgent (`agents/publisher.py`)
- Creates audit_summary (what was checked)
- Documents limitations (what was NOT checked)
- Specifies change_verdict_if (what evidence would change verdict)
- Assigns category_tags for UI navigation (Genesis, Canon, Doctrine, etc.)

### 4. Pipeline Orchestrator (`services/pipeline.py`)
- `PipelineOrchestrator.run_pipeline(question: str)`
- Runs all 5 agents sequentially
- Each agent receives aggregated output from all previous agents
- Fail-fast: Any agent error stops pipeline immediately
- Returns complete result with:
  - Execution timestamps and duration
  - All agent results in order
  - Aggregated `claim_card_data` structure
  - Success/error status with full transparency

### 5. Test Endpoint (`main.py`)
- `POST /api/pipeline/test`
- Accepts `{"question": "..."}`
- Runs complete pipeline
- Returns structured output from all agents
- Does NOT save to database (testing only)
- Shows execution time and per-agent results

---

## File Structure Created

```
src/backend/
├── agents/
│   ├── __init__.py
│   ├── base.py                    # BaseAgent abstract class
│   ├── topic_finder.py            # Agent 1
│   ├── source_checker.py          # Agent 2
│   ├── adversarial_checker.py     # Agent 3
│   ├── writing_agent.py           # Agent 4
│   └── publisher.py               # Agent 5
└── services/
    ├── __init__.py
    ├── llm_client.py              # LLM provider wrapper
    └── pipeline.py                # PipelineOrchestrator
```

---

## Key Design Decisions

1. **Sequential Execution:** Agents run 1→2→3→4→5 with output from N passed to N+1
2. **Fail Fast:** No retries, no defaults, immediate error propagation
3. **Database-Driven Config:** All agent prompts loaded from `agent_prompts` table
4. **Structured JSON Output:** Each agent returns validated JSON/dict
5. **JSON Parsing:** Handles LLM markdown code blocks (```json...```)
6. **Transparency:** Full pipeline trace returned (timestamps, usage stats, all outputs)

---

## What's NOT Included (By Design)

- WebSocket progress updates (Phase 1.5)
- Saving to `claim_cards` table (Phase 2)
- Semantic search / embedding generation (Phase 2)
- Perfect LLM outputs (prompts need tuning in Phase 2)
- Question decomposer (Phase 2 - chat mode)
- Auto-blog generation (Phase 3)

---

## Dependencies

No new dependencies added. Existing packages sufficient:
- `anthropic` - Claude API client
- `openai` - GPT API client
- `fastapi`, `sqlalchemy`, `asyncpg` - Already installed

---

## Testing Notes

**Before testing:**
1. Ensure `agent_prompts` table has entries for all 5 agents:
   - `topic_finder`
   - `source_checker`
   - `adversarial_checker`
   - `writing_agent`
   - `publisher`

2. Set API keys in `.env`:
   - `ANTHROPIC_API_KEY`
   - `OPENAI_API_KEY`

3. Configure each agent in database with:
   - `llm_provider` ("anthropic" or "openai")
   - `model_name` (e.g., "claude-3-opus-20240229", "gpt-4")
   - `system_prompt` (instructions for that agent)
   - `temperature` (0-1)
   - `max_tokens` (e.g., 4096)

**Test endpoint:**
```bash
curl -X POST http://localhost:8008/api/pipeline/test \
  -H "Content-Type: application/json" \
  -d '{"question": "Did the historical Jesus exist?"}'
```

**Expected behavior:**
- Pipeline runs all 5 agents sequentially (~45-60s)
- Returns JSON with full execution trace
- If any agent fails, pipeline stops and returns error with details
- Does NOT save to database (test endpoint only)

---

## Known Issues / Future Work

1. **Agent Prompts Not Seeded:** Need to populate `agent_prompts` table with initial system prompts for each agent
2. **JSON Parsing Robustness:** Current parsing handles markdown code blocks but may need refinement for edge cases
3. **Timeout Configuration:** Uses global `PIPELINE_TIMEOUT` setting; may need per-agent timeouts
4. **Error Messages:** Agent errors expose internal details; may need sanitization for production

---

## Next Steps (Phase 1.5)

1. WebSocket infrastructure for real-time progress updates
2. Per-agent progress events sent to frontend
3. Test with frontend to validate UX flow

---

## Success Criteria ✓

- [x] Base Agent class with config loading from DB
- [x] LLM client wrapper supporting Anthropic + OpenAI
- [x] 5 agent implementations with structured JSON output
- [x] Pipeline orchestrator with sequential execution
- [x] Fail-fast error handling throughout
- [x] Test endpoint for manual pipeline testing
- [x] No new dependencies required

---

## Files Modified

**Created:**
- `src/backend/services/__init__.py`
- `src/backend/services/llm_client.py`
- `src/backend/services/pipeline.py`
- `src/backend/agents/__init__.py`
- `src/backend/agents/base.py`
- `src/backend/agents/topic_finder.py`
- `src/backend/agents/source_checker.py`
- `src/backend/agents/adversarial_checker.py`
- `src/backend/agents/writing_agent.py`
- `src/backend/agents/publisher.py`

**Modified:**
- `src/backend/main.py` (added `POST /api/pipeline/test` endpoint)

**Not Modified:**
- `requirements.txt` (no new dependencies needed)
- `CLAUDE.md` (per user instruction)

---

## Architecture Notes

**Agent Pipeline Flow:**
```
User Question
    ↓
TopicFinderAgent → claim, claimant, claim_type, context
    ↓
SourceCheckerAgent → primary_sources[], scholarly_sources[], evidence_summary
    ↓
AdversarialCheckerAgent → verdict, confidence, apologetics_techniques[]
    ↓
WritingAgent → short_answer, deep_answer, why_persists[]
    ↓
PublisherAgent → audit_summary, limitations, change_verdict_if, category_tags[]
    ↓
Complete ClaimCard Data Structure
```

**Fail-Fast Behavior:**
- Any agent configuration missing → immediate error
- Any LLM call failure → immediate error
- Any JSON parsing failure → immediate error
- Any missing required field → immediate error
- No retries, no defaults, full transparency

**Database Integration:**
- Each agent loads config from `agent_prompts` table at runtime
- Allows prompt tuning without code changes
- Different LLM providers per agent (e.g., Claude for research, GPT for writing)

---

## Session Duration

Approximately 45 minutes (all files created in single session)

---

## End of Session Notes
