# Session: Phase 1.1 - Database Setup

**Date:** 2026-01-12
**Phase:** Foundation - Database Setup
**Status:** Complete ✓

---

## Objectives

Establish database infrastructure for TheReceipts project:
- Create PostgreSQL database and user
- Enable pgvector extension for semantic search
- Design schema for all core entities
- Set up migration system (Alembic)
- Apply initial migration

---

## Completed Tasks

### 1. Database & User Creation

**Database:** `thereceipts_dev`
**User:** `thereceipts`
**Password:** `j6iof0stJ6rUQxJJ--wtZftp` (stored in `/src/backend/.env`)
**Host:** 192.168.50.10:5432 (shared PostgreSQL server)

Created via postgres superuser on database VM:
```sql
CREATE USER thereceipts WITH PASSWORD 'j6iof0stJ6rUQxJJ--wtZftp';
ALTER DATABASE thereceipts_dev OWNER TO thereceipts;
GRANT ALL PRIVILEGES ON DATABASE thereceipts_dev TO thereceipts;
GRANT ALL ON SCHEMA public TO thereceipts;
ALTER SCHEMA public OWNER TO thereceipts;
```

### 2. pgvector Extension

Installed postgresql-16-pgvector on database VM:
```bash
sudo apt update && sudo apt install postgresql-16-pgvector
```

Enabled in thereceipts_dev database:
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

Used for semantic search of existing claim cards (1536-dimension embeddings for OpenAI ada-002).

### 3. Python Backend Setup

**Location:** `/src/backend/`
**Python:** 3.12+ with venv for isolation

Created directory structure:
```
src/backend/
├── venv/                    # Python virtual environment
├── database/
│   ├── __init__.py
│   ├── models.py            # SQLAlchemy models
│   └── migrations/          # Alembic migrations
├── config.py                # Settings and DB connection
├── requirements.txt         # Python dependencies
├── .env                     # Environment variables (gitignored)
├── .env.example             # Template for .env
└── alembic.ini              # Alembic configuration
```

### 4. Database Schema

Designed 5 tables per ADR 001 requirements:

#### claim_cards (core entity)
- `id` (UUID, PK)
- `claim_text` (TEXT) - The claim being audited
- `claimant` (VARCHAR) - Author/org who made the claim
- `claim_type` (VARCHAR) - history, science, doctrine, translation, etc.
- `verdict` (ENUM) - True | Misleading | False | Unfalsifiable | Depends on Definitions
- `short_answer` (TEXT) - ≤150 words plain-language summary
- `deep_answer` (TEXT) - Full detailed analysis
- `why_persists` (JSONB) - Psychological/social/institutional reasons (array)
- `confidence_level` (ENUM) - High | Medium | Low
- `confidence_explanation` (TEXT) - Why this confidence level
- `agent_audit` (JSONB) - Full pipeline execution trace
- `embedding` (VECTOR[1536]) - For semantic search
- `created_at`, `updated_at` (TIMESTAMP)
- **Indexes:** claim_text, claimant, verdict, created_at

#### sources (linked to claim_cards)
- `id` (UUID, PK)
- `claim_card_id` (UUID, FK → claim_cards.id)
- `source_type` (ENUM) - primary historical | scholarly peer-reviewed
- `citation` (TEXT) - Full citation
- `url` (TEXT, nullable) - Link if available
- `quote_text` (TEXT, nullable) - Relevant quote
- `created_at` (TIMESTAMP)
- **Indexes:** claim_card_id, source_type

#### apologetics_tags (linked to claim_cards)
- `id` (UUID, PK)
- `claim_card_id` (UUID, FK → claim_cards.id)
- `technique_name` (VARCHAR) - e.g., quote-mining, false dichotomy
- `description` (TEXT, nullable) - How this technique was used
- `created_at` (TIMESTAMP)
- **Indexes:** claim_card_id, technique_name

#### agent_prompts (editable LLM configurations)
- `id` (UUID, PK)
- `agent_name` (VARCHAR, UNIQUE) - topic_finder, source_checker, etc.
- `llm_provider` (VARCHAR) - anthropic, openai, etc.
- `model_name` (VARCHAR) - claude-3-opus, gpt-4, etc.
- `system_prompt` (TEXT) - Full system prompt
- `temperature` (FLOAT, default 0.7)
- `max_tokens` (INTEGER, default 4096)
- `created_at`, `updated_at` (TIMESTAMP)
- **Indexes:** agent_name

#### topic_queue (auto-blog generation queue)
- `id` (UUID, PK)
- `topic_text` (TEXT) - Topic/claim to audit
- `priority` (INTEGER) - Higher = process sooner
- `status` (ENUM) - queued | processing | completed | failed
- `source` (VARCHAR, nullable) - Where topic came from (manual, auto-suggest)
- `claim_card_ids` (ARRAY[VARCHAR], nullable) - Generated claim card UUIDs
- `scheduled_for` (TIMESTAMP, nullable)
- `error_message` (TEXT, nullable) - For failed topics
- `retry_count` (INTEGER, default 0)
- `created_at`, `updated_at` (TIMESTAMP)
- **Indexes:** status, priority, scheduled_for

### 5. SQLAlchemy Models

**File:** `src/backend/database/models.py`

Implemented full ORM models with:
- Type safety via Pydantic enums
- Relationships (claim_cards → sources, apologetics_tags)
- UUID primary keys (using uuid.uuid4)
- JSONB for flexible data (agent_audit, why_persists)
- pgvector integration for embeddings
- Cascade deletes for child records

### 6. Configuration Management

**File:** `src/backend/config.py`

Uses pydantic-settings for type-safe config:
- Loads from `.env` file
- Database connection URL builders (sync + async)
- LLM API keys (Anthropic, OpenAI)
- Service configuration (host, port)
- Pipeline settings (timeout, semantic search threshold)

### 7. Migration System

**Tool:** Alembic
**Location:** `src/backend/database/migrations/`

Configured to:
- Auto-import models from `database.models`
- Load database URL from `config.py`
- Support autogenerate for schema changes

Initial migration created:
- **Revision:** 597a35a22323
- **Description:** Initial schema (all 5 tables)
- **Status:** Applied successfully ✓

### 8. Verification

Confirmed all tables created:
```
 public | agent_prompts    | table | thereceipts
 public | alembic_version  | table | thereceipts
 public | apologetics_tags | table | thereceipts
 public | claim_cards      | table | thereceipts
 public | sources          | table | thereceipts
 public | topic_queue      | table | thereceipts
```

---

## Files Created

```
src/backend/
├── venv/                                           # Virtual environment
├── database/
│   ├── __init__.py                                # Package exports
│   ├── models.py                                  # SQLAlchemy models (282 lines)
│   └── migrations/
│       ├── env.py                                 # Alembic environment (modified)
│       ├── README                                 # Alembic README
│       ├── script.py.mako                         # Migration template
│       └── versions/
│           └── 597a35a22323_initial_schema_*.py   # Initial migration
├── config.py                                      # Configuration management (64 lines)
├── requirements.txt                               # Python dependencies
├── .env                                           # Environment variables (with credentials)
├── .env.example                                   # Template for .env
└── alembic.ini                                    # Alembic configuration
```

---

## Dependencies Installed

Core framework:
- fastapi==0.109.0
- uvicorn[standard]==0.27.0
- pydantic==2.5.3
- pydantic-settings==2.1.0

Database:
- sqlalchemy==2.0.25
- asyncpg==0.29.0 (async PostgreSQL driver)
- psycopg2-binary==2.9.9 (sync PostgreSQL driver)
- alembic==1.13.1
- pgvector==0.2.4

LLM providers:
- anthropic==0.25.1
- openai==1.12.0

WebSocket:
- websockets==12.0

Utilities:
- python-dotenv==1.0.0

---

## Key Design Decisions

### 1. UUID Primary Keys
- Better for distributed systems
- No sequential ID leakage
- Allows client-side ID generation if needed

### 2. JSONB for Flexible Data
- `agent_audit`: Full pipeline trace (varies by agent)
- `why_persists`: Array of reasons (variable length)
- Queryable via PostgreSQL JSONB operators

### 3. pgvector Integration
- 1536-dimension vectors (OpenAI ada-002 standard)
- Enables fast semantic similarity search
- Threshold: 0.85 similarity for cache hits (per ADR 001)

### 4. Separate Sources Table
- Clear distinction: primary historical vs scholarly peer-reviewed
- One claim can have multiple sources
- Queryable independently for source analysis

### 5. Editable Agent Prompts
- System prompts stored in database (not hardcoded)
- Admin can adjust without code changes
- Per-agent LLM provider configuration

### 6. Fail-Fast Topic Queue
- No automatic retries (per ADR 001 fail-fast principle)
- Error messages preserved for manual review
- Status tracking for admin portal

---

## Database Connection Details

**Connection String:**
```
postgresql://thereceipts:j6iof0stJ6rUQxJJ--wtZftp@192.168.50.10:5432/thereceipts_dev
```

**Async Connection String (for FastAPI):**
```
postgresql+asyncpg://thereceipts:j6iof0stJ6rUQxJJ--wtZftp@192.168.50.10:5432/thereceipts_dev
```

**Credentials stored in:** `src/backend/.env` (gitignored)
**Template provided in:** `src/backend/.env.example`

---

## Migration Commands

```bash
# Activate virtual environment
cd src/backend
source venv/bin/activate

# Create new migration (after model changes)
alembic revision --autogenerate -m "Description of changes"

# Apply migrations
alembic upgrade head

# Rollback one migration
alembic downgrade -1

# Show current migration version
alembic current

# Show migration history
alembic history
```

---

## Issues Encountered & Resolved

### 1. Permission Error Creating Database
**Issue:** `odin` user lacked CREATE DATABASE privilege
**Solution:** Used `aimee_user` credentials (has CREATEDB attribute)

### 2. pgvector Extension Not Available
**Issue:** pgvector not installed on database VM
**Solution:** User installed `postgresql-16-pgvector` on VM

### 3. Migration Import Error
**Issue:** Alembic-generated migration missing `pgvector` import
**Solution:** Manually added `from pgvector.sqlalchemy import Vector` to migration file

---

## Next Steps (Phase 1.2)

1. **FastAPI Application Scaffold**
   - Main application entry point (`main.py`)
   - Database session management
   - Basic health check endpoint

2. **Database Access Layer**
   - Repository pattern for each model
   - Async database operations
   - Connection pooling configuration

3. **Initial Agent Prompt Seeding**
   - Create seed data for 5 agents
   - Default system prompts
   - LLM provider configurations

4. **Testing Infrastructure**
   - pytest setup
   - Database fixtures
   - Test database configuration

---

## Schema Alignment with ADR 001

✓ All fields from ADR 001 Section 5 (Data Model) implemented
✓ Five agent pipeline support (agent_prompts table)
✓ Semantic search infrastructure (embedding + pgvector)
✓ Auto-blog system support (topic_queue table)
✓ Source separation (primary vs scholarly)
✓ Apologetics technique tracking
✓ Full audit trail storage (agent_audit JSONB)
✓ Confidence levels with explanations

---

## Notes

- Database is on shared server (192.168.50.10) - same as Odin project
- Separate database and user isolate this project
- pgvector extension required for semantic search (installed on VM)
- Migration system ready for iterative schema evolution
- Config uses pydantic-settings for type safety and validation
- LLM API keys copied from Odin project for now (can be updated)

Session complete. Database foundation ready for Phase 1.2 (FastAPI scaffold).
